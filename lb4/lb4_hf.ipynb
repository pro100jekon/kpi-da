{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T02:25:49.110328Z",
     "start_time": "2025-01-14T02:25:48.905058Z"
    }
   },
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Path to the GGUF model file\n",
    "model_path = \"./Mistral-7B-Instruct-Ukrainian.Q8_0.gguf\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T02:32:22.466851Z",
     "start_time": "2025-01-14T02:25:49.116413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "llm = Llama(model_path=model_path, n_gpu_layers=20)\n",
    "\n",
    "# Input prompt\n",
    "prompt = \"Найкращий клас для гри у World of Warcraft?\"\n",
    "\n",
    "# Generate response\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    repeat_penalty=1.1\n",
    ")\n",
    "\n",
    "# Print the model's response\n",
    "print(output[\"choices\"][0][\"text\"])\n"
   ],
   "id": "51d87f5331e853e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 38 key-value pairs and 291 tensors from ./Mistral-7B-Instruct-Ukrainian.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = MistralUkrainian2.0Merge\n",
      "llama_model_loader: - kv   3:                       general.organization str              = RaduGabriel\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-Ukrainian\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Mistral\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   7:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  17:                           llama.vocab_size u32              = 32000\n",
      "llama_model_loader: - kv  18:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.scores arr[f32,32000]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,32000]   = [3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  26:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  28:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  31:                                general.url str              = https://huggingface.co/mradermacher/M...\n",
      "llama_model_loader: - kv  32:              mradermacher.quantize_version str              = 2\n",
      "llama_model_loader: - kv  33:                  mradermacher.quantized_by str              = mradermacher\n",
      "llama_model_loader: - kv  34:                  mradermacher.quantized_at str              = 2024-12-21T11:31:18+01:00\n",
      "llama_model_loader: - kv  35:                  mradermacher.quantized_on str              = nico1\n",
      "llama_model_loader: - kv  36:                         general.source.url str              = https://huggingface.co/SherlockAssist...\n",
      "llama_model_loader: - kv  37:                  mradermacher.convert_type str              = hf\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: control token:      0 '<unk>' is not marked as EOG\n",
      "llm_load_vocab: control token:      2 '</s>' is not marked as EOG\n",
      "llm_load_vocab: control token:      1 '<s>' is not marked as EOG\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 7.17 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = MistralUkrainian2.0Merge\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =  7338.64 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 512\n",
      "llama_new_context_with_model: n_ctx_per_seq = 512\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 1000000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'mradermacher.quantized_on': 'nico1', 'mradermacher.quantized_by': 'mradermacher', 'mradermacher.quantize_version': '2', 'mradermacher.quantized_at': '2024-12-21T11:31:18+01:00', 'llama.embedding_length': '4096', 'mradermacher.convert_type': 'hf', 'general.source.url': 'https://huggingface.co/SherlockAssistant/Mistral-7B-Instruct-Ukrainian', 'llama.feed_forward_length': '14336', 'general.license': 'apache-2.0', 'tokenizer.ggml.add_bos_token': 'true', 'general.size_label': '7B', 'general.type': 'model', 'llama.context_length': '32768', 'general.name': 'MistralUkrainian2.0Merge', 'tokenizer.ggml.bos_token_id': '1', 'general.basename': 'Mistral', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.architecture': 'llama', 'general.url': 'https://huggingface.co/mradermacher/Mistral-7B-Instruct-Ukrainian-GGUF', 'llama.rope.freq_base': '1000000.000000', 'general.finetune': 'Instruct-Ukrainian', 'general.file_type': '7', 'tokenizer.ggml.pre': 'default', 'llama.vocab_size': '32000', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'general.organization': 'RaduGabriel', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n",
      "llama_perf_context_print:        load time =    2676.66 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    17 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   47092.25 ms /   216 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Підбір класу важливий, щоб мати успіх.\n",
      "World of Warcraft - це відома MMORPG, яка захоплює гравців з усього світу. Однак, якщо ви починаєте грати в цю гру, то може здатися складно підібрати собі ідеальний клас для себе, який відповідає вашим цілям. У цій статті ми намагаємось розкрити деякі основні риси кожного класу, щоб вам було легше визначитися з вибором.\n",
      "\n",
      "Варто зауважити, що існує вісім основних класів у WoW: Войни, Чорнокнижники (Шамани), Лицарі\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T13:33:03.905223Z",
     "start_time": "2025-01-14T11:50:22.551065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"SherlockAssistant/Mistral-7B-Instruct-Ukrainian\"\n",
    "q1 = [{\"role\": \"user\", \"content\": \"Найкращий клас для гри у World of Warcraft?\"}]\n",
    "q2 = [{\"role\": \"user\", \"content\": \"Я граю на друїді, це гарний хіл для ключів?\"}]\n",
    "q3 = [{\"role\": \"user\", \"content\": \"А які здібності є в нього?\"}]\n",
    "\n",
    "# m = AutoModel.from_pretrained(model).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "p1, p2, p3 = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True, offload_buffers=True) for m in [q1, q2, q3]]\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "o1, o2, o3 = [pipeline(prompt, max_new_tokens=512, do_sample=True, temperature=0.7, top_k=70, top_p=0.95) for prompt in [p1, p2, p3]]"
   ],
   "id": "ec0ffe6b408dc726",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.68it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T13:54:39.478089Z",
     "start_time": "2025-01-14T13:54:39.427028Z"
    }
   },
   "cell_type": "code",
   "source": "o1",
   "id": "6e828f28f2879ba9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<s>[INST] Найкращий клас для гри у World of Warcraft? [/INST] Вибір класу в World of Warcraft залежить від того, як гравець планує грати. Найкращий клас залежить від його стилю гри та його цілей. Деякі класи більш схильні до бою одного на одного, тоді як інші кращі для підтримки групи або далекобійних атак. Є класи, які відмінно підходять для лідерів групи, а інші є більш спеціалізованими на конкретні ролі, такі як лікування, танкинг, а також допомога, підтримка та бойові класи. Гравці повинні враховувати ці аспекти, коли обирають клас, щоб задовольнити свої потреби в грі. У World of Warcraft є 12 основних класів, кожен з яких має унікальні здібності, і ігрові механіки, що роблять його цікавим і привабливим. Найкращий клас - це той, який найкраще відповідає стилю гри та цілям кожного гравця окремо. Існує багато порад та оглядів, які можна знайти в Інтернеті, щоб допомогти почати, але найкращим способом визначити найкращий клас - це спробувати їх самим. Гравці можуть перемикатися між класами, щоб знайти той, який найкраще відповідає їхньому стилю гри. Це не лише дозволяє гравцям отримати розуміння про те, які класи їм подобаються, але й дозволяє їм виявити, які сильні та слабкі сторони у кожному класі. Крім того, ігрові механіки можуть змінюватися з часом, що додає різно'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T13:55:01.676614Z",
     "start_time": "2025-01-14T13:55:01.670814Z"
    }
   },
   "cell_type": "code",
   "source": "o2",
   "id": "fc33db2df620f80f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<s>[INST] Я граю на друїді, це гарний хіл для ключів? [/INST] Досить часто, Druid (Друїд) вважається одним з найкращих класів для ключів, оскільки він має багато різноманітних інструментів, щоб контролювати боїв і підтримувати партію. Посилання на життя, контроль над елементами та бойові здібності, як-от Ферментування, роблять його корисним для різних ситуацій. Хороший гравець на друїді може значно підвищити шанси на успішну проходження ключів. Однак, важливо зазначити, що успіх у грі Warcraft залежить не лише від вибору класу, а й від знань, досвіду та навичок гравця, тому що гра на будь-якому класі може бути ефективною, якщо гравець грає на ньому добре. Тому вибір класу не є вирішальним чинником. Важливою частиною успіху є правильні стратегії та розуміння механіки гри. Тому, якщо у вас є бажання грати на друїді, то це можливо, але рекомендується також спробувати інші класи і визначити, які з них підходять вам найбільше. У кінцевому підсумку, важливо, що вам сподобається грати і ви могли б досягати успіху на будь-якому класі. #druid #warcraft #клас #геймінг #класи #класовий вибір #ключі #ключі в #клонінгу #клас Druid #геймінг #класовий вибір #ключі #ключі в #клонінгу #клас Druid #ключі #класовий вибір #клас #Warcraft #gaming #Druid #клас #ключі в #клонін'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T13:55:03.422913Z",
     "start_time": "2025-01-14T13:55:03.417741Z"
    }
   },
   "cell_type": "code",
   "source": "o3",
   "id": "de3886667c3805c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<s>[INST] А які здібності є в нього? [/INST] Я не можу впевнено сказати, які конкретні здібності у вас є, оскільки я не маю доступу до ваших особистих даних. Однак, людські здібності зазвичай включають в себе різноманітні навички, навички, таланти та інтелектуальні якості, такі як критичне мислення, творчість, фізична сила, розумова здатність, спортивні навички та соціальні навички. Якби ви надали більше деталей про себе, я міг би краще відповісти на це запитання. Наприклад, якщо ви маєте хист до музики, ми могли б розмовляти про ваші музичні здібності, якщо ви любите малювати, то ми могли б обговорювати ваш художній талант, або якщо ви є успішним спортсменом, то ми могли б зосереджуватися на вашій фізичній силі та координації. У будь-якому разі, здібності - це те, що робить вас унікальним і особливим, і вони виявляються в різних аспектах вашого життя. Продовжуйте розвиватися і відкривати нові можливості, щоб використовувати свої здібності максимально ефективно! #здібності #таланти #інтелектуальні_якості #критичне_мислення #творчість #фізична_сила #розумова_здатність #спортивні_навички #соціальні_навички #унікальність #розвиток #особливість #відкриття #потенціал #навички #освіта #знання #навчання #розвиток_особистості'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
